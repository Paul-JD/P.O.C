{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:32.093862400Z",
     "start_time": "2023-08-31T19:53:31.344198600Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "from collections import Counter\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "url = \"https://files.data.gouv.fr/geo-dvf/latest/csv/2018/full.csv.gz\"\n",
    "req = requests.get(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:33.827018400Z",
     "start_time": "2023-08-31T19:53:32.099849200Z"
    }
   },
   "id": "2ee85954a7585345"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "t = req.content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:33.843018400Z",
     "start_time": "2023-08-31T19:53:33.830052400Z"
    }
   },
   "id": "a0f8f2266c261484"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "s = gzip.decompress(t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:36.183298700Z",
     "start_time": "2023-08-31T19:53:33.845020Z"
    }
   },
   "id": "ed9fea5b0affa2d1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dargo\\AppData\\Local\\Temp\\ipykernel_19344\\798997524.py:1: DtypeWarning: Columns (8,10,12,14,16,17,18,20,22,26,35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(io.BytesIO(s))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(io.BytesIO(s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:50.082457500Z",
     "start_time": "2023-08-31T19:53:36.188299500Z"
    }
   },
   "id": "932b5e490983f2cc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# suppression valeur fonciere nulle\n",
    "df.dropna(subset=['valeur_fonciere'], inplace=True)\n",
    "df.dropna(subset=['longitude', 'latitude'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:52.640817500Z",
     "start_time": "2023-08-31T19:53:50.085422800Z"
    }
   },
   "id": "8a4eef8c0ec9fe2c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Selection des colonnes utiles\n",
    "dataset = df[['id_mutation',\n",
    "              'date_mutation',\n",
    "              'nature_mutation',\n",
    "              'adresse_nom_voie',\n",
    "              'code_postal',\n",
    "              'code_commune',\n",
    "              'code_departement',\n",
    "              'nombre_lots',\n",
    "              'type_local',\n",
    "              'surface_reelle_bati',\n",
    "              'nombre_pieces_principales',\n",
    "              'code_nature_culture',\n",
    "              'code_nature_culture_speciale',\n",
    "              'surface_terrain',\n",
    "              'valeur_fonciere',\n",
    "              'longitude',\n",
    "              'latitude'\n",
    "              ]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:53.019121100Z",
     "start_time": "2023-08-31T19:53:52.644818700Z"
    }
   },
   "id": "e56e372dae07f21d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:53.466850300Z",
     "start_time": "2023-08-31T19:53:53.022123100Z"
    }
   },
   "id": "b5c88c697d2a8201"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c6572facca896a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(3201638, 17)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:53:53.480844400Z",
     "start_time": "2023-08-31T19:53:53.468845100Z"
    }
   },
   "id": "fe6ac2634226c233"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date mutation cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da7ef45987f3d39"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Passage de la colonnes date mutation en datetime puis en seconde pour avoir des int\n",
    "dataset.loc[:, 'date_mutation'] = pd.to_datetime(dataset['date_mutation'], format='mixed')\n",
    "dataset.loc[:, 'date_mutation'] = dataset['date_mutation'].apply(lambda x: x.timestamp())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:04.056522100Z",
     "start_time": "2023-08-31T19:53:53.480844400Z"
    }
   },
   "id": "410b6bd7489f0c65"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Remplissage des valeurs NaN nature_mutation et labelisation\n",
    "dataset.loc[:, 'nature_mutation'] = dataset.nature_mutation.fillna(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:04.276535800Z",
     "start_time": "2023-08-31T19:54:04.059558900Z"
    }
   },
   "id": "55ea0df08ab8429f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NM = preprocessing.LabelEncoder()\n",
    "NM.fit(dataset['nature_mutation'])\n",
    "dataset.loc[:, 'nature_mutation'] = NM.transform(dataset['nature_mutation'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bbe127e6e9d3727"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adresse nom voie cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc749794b07b0da3"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "abrv_voie = pd.read_csv(filepath_or_buffer='Data_Files/ABREVIATION_VOIE.csv', sep=',').values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:04.322565300Z",
     "start_time": "2023-08-31T19:54:04.280527900Z"
    }
   },
   "id": "8465c1c51bdec236"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "dataset.loc[:, 'adresse_nom_voie'] = dataset.adresse_nom_voie.fillna(\"\")\n",
    "\n",
    "codex_voie = list()\n",
    "for i in dataset.adresse_nom_voie.values:\n",
    "\n",
    "    list_nom_rue = i.split(' ')\n",
    "    premier_valeur_liste = list_nom_rue[0]\n",
    "\n",
    "    if premier_valeur_liste in abrv_voie:\n",
    "        codex_voie.append(premier_valeur_liste)\n",
    "    else:\n",
    "        codex_voie.append('AUTRE')\n",
    "\n",
    "dataset.loc[:, 'prefixe_voie'] = codex_voie"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:26.987334900Z",
     "start_time": "2023-08-31T19:54:04.309520100Z"
    }
   },
   "id": "1109a459bcdc1b60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "CV = preprocessing.LabelEncoder()\n",
    "CV.fit(dataset.prefixe_voie)\n",
    "dataset.loc[:,'prefixe_voie'] = CV.transform(dataset.prefixe_voie)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9df4c03294ab1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multiple easy Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52995de51908ce9"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "dataset.loc[:, 'code_postal'] = dataset.code_postal.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:27.080664Z",
     "start_time": "2023-08-31T19:54:27.023902600Z"
    }
   },
   "id": "9a31b10f690da8e7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "dataset.loc[:, 'surface_reelle_bati'] = dataset.surface_reelle_bati.fillna(0)\n",
    "dataset.loc[:, 'surface_terrain'] = dataset.surface_terrain.fillna(0)\n",
    "dataset.loc[:, 'type_local'] = dataset.type_local.fillna('Autre')\n",
    "dataset.loc[:, 'nombre_pieces_principales'] = dataset.nombre_pieces_principales.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:27.443615600Z",
     "start_time": "2023-08-31T19:54:27.085627400Z"
    }
   },
   "id": "6b61f19cf0aceb"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "dataset.loc[:, 'code_nature_culture'] = dataset.code_nature_culture.fillna(\"\")\n",
    "dataset.loc[:, 'code_nature_culture_speciale'] = dataset.code_nature_culture_speciale.fillna(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:27.856906800Z",
     "start_time": "2023-08-31T19:54:27.446651400Z"
    }
   },
   "id": "5f406383cd6b6ab1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Latitude & longitude"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8987fac75112dc2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construction data set modele"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b049722fe2a8e209"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "classe_liste_nature_mutation = list(dict.fromkeys(list(dataset.nature_mutation.values)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.003133300Z",
     "start_time": "2023-08-31T19:54:27.861904Z"
    }
   },
   "id": "d736eb863bc08db4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "classe_liste_code_type_local = list(dict.fromkeys(list(dataset.type_local.values)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.145535400Z",
     "start_time": "2023-08-31T19:54:28.000135400Z"
    }
   },
   "id": "920cd24c3eaf4b67"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "classe_liste_prefixe_voie = list(dict.fromkeys(codex_voie))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.263657500Z",
     "start_time": "2023-08-31T19:54:28.141534Z"
    }
   },
   "id": "cc7fa6d25879e980"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "classe_liste_code_culture = pd.read_csv(filepath_or_buffer='Data_Files/CODE_CULTURE.csv', sep=',')\n",
    "classe_liste_code_culture = list(classe_liste_code_culture['Code_nature_culture'].to_dict().values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.326935600Z",
     "start_time": "2023-08-31T19:54:28.265605900Z"
    }
   },
   "id": "9b0c72d63c8ef10b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "classe_liste_code_culture_spe = pd.read_csv(filepath_or_buffer='Data_Files/CODE_CULTURE_SPECIALE.csv', sep=',')\n",
    "classe_liste_code_culture_spe = list(classe_liste_code_culture_spe['CODE_CULTURE_SPECIALE'].to_dict().values())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.342934300Z",
     "start_time": "2023-08-31T19:54:28.329944300Z"
    }
   },
   "id": "241ad3a1371acdaa"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "Nom_colonnes_from_dataset = [\n",
    "    'date_mutation',\n",
    "    'code_postal',\n",
    "    'code_commune',\n",
    "    'code_departement',\n",
    "    'nombre_lots',\n",
    "    'surface_reelle_bati',\n",
    "    'nombre_pieces_principales',\n",
    "    'surface_terrain',\n",
    "    'valeur_fonciere',\n",
    "    'longitude',\n",
    "    'latitude'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.395306500Z",
     "start_time": "2023-08-31T19:54:28.346973300Z"
    }
   },
   "id": "c3aa3976be10794f"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "colonnes = Nom_colonnes_from_dataset + classe_liste_code_type_local + classe_liste_prefixe_voie + classe_liste_nature_mutation + classe_liste_code_culture + classe_liste_code_culture_spe\n",
    "colonnes = np.array(colonnes)\n",
    "del  classe_liste_nature_mutation, classe_liste_code_type_local, classe_liste_prefixe_voie, classe_liste_code_culture, classe_liste_code_culture_spe"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.430087900Z",
     "start_time": "2023-08-31T19:54:28.398296700Z"
    }
   },
   "id": "9b3dbe4c60676100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "assert 1 == len(pd.unique(TEST.date_mutation))\n",
    "assert 1 == len(pd.unique(TEST.valeur_fonciere))\n",
    "assert 1 == len(pd.unique(TEST.code_postal))\n",
    "assert 1 == len(pd.unique(TEST.code_commune))\n",
    "assert 1 == len(pd.unique(TEST.code_departement))\n",
    "assert len(ajout_culture_spe) == len(classe_liste_code_culture_spe)\n",
    "assert len(ajout_culture) == len(classe_liste_code_culture)\n",
    "assert len(ajout_nature_mutation) == len(classe_liste_nature_mutation)\n",
    "assert len(ajout_abbrev_voie) == len(classe_liste_prefixe_voie)\n",
    "assert len(ajout_code_type) == len(classe_liste_code_type_local)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5da16b1c1a340466"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset creation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d44b41cf16f43474"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "test = dataset.loc[0:100000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.483803100Z",
     "start_time": "2023-08-31T19:54:28.424076900Z"
    }
   },
   "id": "85ca41a00a0ffa5d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "list_id_mutation = list(dict.fromkeys(test.id_mutation.values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.538000100Z",
     "start_time": "2023-08-31T19:54:28.485806400Z"
    }
   },
   "id": "8b5227181774eebd"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class My_thread(Thread):\n",
    "    def __init__(self, id, dataset, colonnes):\n",
    "        super(My_thread,self).__init__()\n",
    "        self.id = id\n",
    "        self.dataset = dataset\n",
    "        self.colonnes = colonnes\n",
    "        self.retour = pd.DataFrame()\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        tmp_dataset = self.dataset\n",
    "        index = tmp_dataset.index[0]\n",
    "        tmp_dict = dict.fromkeys(self.colonnes)\n",
    "    \n",
    "        tmp_dict['date_mutation'] = [tmp_dataset.loc[index, 'date_mutation']]\n",
    "        tmp_dict['code_postal'] = [tmp_dataset.loc[index, 'code_postal']]\n",
    "        tmp_dict['code_commune'] = [tmp_dataset.loc[index, 'code_commune']]\n",
    "        tmp_dict['code_departement'] = [tmp_dataset.loc[index, 'code_departement']]\n",
    "        tmp_dict['nombre_lots'] = [tmp_dataset.loc[index, 'nombre_lots'].sum()]\n",
    "        tmp_dict['surface_reelle_bati'] = [tmp_dataset.loc[index, 'surface_reelle_bati'].sum()]\n",
    "        tmp_dict['nombre_pieces_principales'] = [tmp_dataset.loc[index, 'nombre_pieces_principales'].sum()]\n",
    "        tmp_dict['surface_terrain'] = [tmp_dataset.loc[index, 'surface_terrain'].sum()]\n",
    "        tmp_dict['valeur_fonciere'] = [tmp_dataset.loc[index, 'valeur_fonciere']]\n",
    "        tmp_dict['longitude'] = [tmp_dataset.loc[index, 'longitude']]\n",
    "        tmp_dict['latitude'] = [tmp_dataset.loc[index, 'latitude']]\n",
    "    \n",
    "        # valeur classe code type local\n",
    "        count_type_local = Counter(tmp_dataset.loc[:, 'type_local'])\n",
    "        for type_local in count_type_local:\n",
    "            tmp_dict[type_local] = count_type_local[type_local]\n",
    "    \n",
    "        # valeur prefixe voie\n",
    "        count_prefixe_voie = Counter(tmp_dataset.loc[:, 'prefixe_voie'])\n",
    "        for abbrev_voie in count_prefixe_voie:\n",
    "            tmp_dict[abbrev_voie] = count_prefixe_voie[abbrev_voie]\n",
    "    \n",
    "        # valeur classe nature mutation\n",
    "        count_nature_mutation = Counter(tmp_dataset.loc[:, 'nature_mutation'])\n",
    "        for nat_mutation in count_nature_mutation:\n",
    "            tmp_dict[nat_mutation] = count_nature_mutation[nat_mutation]\n",
    "    \n",
    "        # valeur classe culture\n",
    "        count_type_culture = Counter(tmp_dataset.loc[:, 'code_nature_culture'])\n",
    "        for culture in count_type_culture:\n",
    "            tmp_dict[culture] = count_type_culture[culture]\n",
    "    \n",
    "        # valeur classe culture\n",
    "        count_type_culture_spe = Counter(tmp_dataset.loc[:, 'code_nature_culture_speciale'])\n",
    "        for culture_spe in count_type_culture_spe:\n",
    "            tmp_dict[culture_spe] = count_type_culture_spe[culture_spe]\n",
    "        \n",
    "        self.retour = pd.DataFrame.from_dict(tmp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.595877800Z",
     "start_time": "2023-08-31T19:54:28.538000100Z"
    }
   },
   "id": "65f1a24386edfc"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "th_liste = list(range(len(list_id_mutation)))\n",
    "Data = pd.DataFrame()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T19:54:28.607876800Z",
     "start_time": "2023-08-31T19:54:28.594874700Z"
    }
   },
   "id": "e18b9f36104b42e1"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "for i, id in enumerate(list_id_mutation):\n",
    "    tmp = test.loc[test.id_mutation == id,:]\n",
    "    th_liste[i] : My_thread = My_thread(id, tmp, colonnes)\n",
    "    th_liste[i].start()\n",
    "    test = test.drop(test[test.id_mutation == id].index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T20:05:32.437848200Z",
     "start_time": "2023-08-31T19:54:28.608884100Z"
    }
   },
   "id": "b59acc42641f47f3"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(th_liste) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m      2\u001B[0m     th_liste[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mjoin()\n\u001B[1;32m----> 3\u001B[0m     Data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mData\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mth_liste\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretour\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m th_liste[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:393\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    378\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    380\u001B[0m op \u001B[38;5;241m=\u001B[39m _Concatenator(\n\u001B[0;32m    381\u001B[0m     objs,\n\u001B[0;32m    382\u001B[0m     axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    390\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m    391\u001B[0m )\n\u001B[1;32m--> 393\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001B[0m, in \u001B[0;36m_Concatenator.get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m             indexers[ax] \u001B[38;5;241m=\u001B[39m obj_labels\u001B[38;5;241m.\u001B[39mget_indexer(new_labels)\n\u001B[0;32m    678\u001B[0m     mgrs_indexers\u001B[38;5;241m.\u001B[39mappend((obj\u001B[38;5;241m.\u001B[39m_mgr, indexers))\n\u001B[1;32m--> 680\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mconcatenate_managers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcat_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbm_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    684\u001B[0m     new_data\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\internals\\concat.py:189\u001B[0m, in \u001B[0;36mconcatenate_managers\u001B[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001B[0m\n\u001B[0;32m    187\u001B[0m     fastpath \u001B[38;5;241m=\u001B[39m blk\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m values\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 189\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[43m_concatenate_join_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjoin_units\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    190\u001B[0m     fastpath \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fastpath:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\internals\\concat.py:466\u001B[0m, in \u001B[0;36m_concatenate_join_units\u001B[1;34m(join_units, copy)\u001B[0m\n\u001B[0;32m    463\u001B[0m has_none_blocks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28many\u001B[39m(unit\u001B[38;5;241m.\u001B[39mblock\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mV\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m unit \u001B[38;5;129;01min\u001B[39;00m join_units)\n\u001B[0;32m    464\u001B[0m upcasted_na \u001B[38;5;241m=\u001B[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001B[1;32m--> 466\u001B[0m to_concat \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    467\u001B[0m     ju\u001B[38;5;241m.\u001B[39mget_reindexed_values(empty_dtype\u001B[38;5;241m=\u001B[39mempty_dtype, upcasted_na\u001B[38;5;241m=\u001B[39mupcasted_na)\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ju \u001B[38;5;129;01min\u001B[39;00m join_units\n\u001B[0;32m    469\u001B[0m ]\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(is_1d_only_ea_dtype(t\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m to_concat):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001B[39;00m\n\u001B[0;32m    476\u001B[0m     to_concat \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    477\u001B[0m         t\n\u001B[0;32m    478\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m is_1d_only_ea_dtype(t\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    479\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m t[\u001B[38;5;241m0\u001B[39m, :]  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m to_concat\n\u001B[0;32m    481\u001B[0m     ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\internals\\concat.py:467\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    463\u001B[0m has_none_blocks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28many\u001B[39m(unit\u001B[38;5;241m.\u001B[39mblock\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mV\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m unit \u001B[38;5;129;01min\u001B[39;00m join_units)\n\u001B[0;32m    464\u001B[0m upcasted_na \u001B[38;5;241m=\u001B[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001B[0;32m    466\u001B[0m to_concat \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 467\u001B[0m     \u001B[43mju\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_reindexed_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mempty_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mempty_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupcasted_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mupcasted_na\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    468\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ju \u001B[38;5;129;01min\u001B[39;00m join_units\n\u001B[0;32m    469\u001B[0m ]\n\u001B[0;32m    471\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(is_1d_only_ea_dtype(t\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m to_concat):\n\u001B[0;32m    472\u001B[0m     \u001B[38;5;66;03m# TODO(EA2D): special case not needed if all EAs used HybridBlocks\u001B[39;00m\n\u001B[0;32m    473\u001B[0m \n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# error: No overload variant of \"__getitem__\" of \"ExtensionArray\" matches\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;66;03m# argument type \"Tuple[int, slice]\"\u001B[39;00m\n\u001B[0;32m    476\u001B[0m     to_concat \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    477\u001B[0m         t\n\u001B[0;32m    478\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m is_1d_only_ea_dtype(t\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    479\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m t[\u001B[38;5;241m0\u001B[39m, :]  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[0;32m    480\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m to_concat\n\u001B[0;32m    481\u001B[0m     ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\internals\\concat.py:452\u001B[0m, in \u001B[0;36mJoinUnit.get_reindexed_values\u001B[1;34m(self, empty_dtype, upcasted_na)\u001B[0m\n\u001B[0;32m    449\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m values\u001B[38;5;241m.\u001B[39msize \u001B[38;5;129;01mand\u001B[39;00m values[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    450\u001B[0m             fill_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 452\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmake_na_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mempty_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProjectPOC\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2325\u001B[0m, in \u001B[0;36mmake_na_array\u001B[1;34m(dtype, shape, fill_value)\u001B[0m\n\u001B[0;32m   2321\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m missing_arr\n\u001B[0;32m   2322\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2323\u001B[0m     \u001B[38;5;66;03m# NB: we should never get here with dtype integer or bool;\u001B[39;00m\n\u001B[0;32m   2324\u001B[0m     \u001B[38;5;66;03m#  if we did, the missing_arr.fill would cast to gibberish\u001B[39;00m\n\u001B[1;32m-> 2325\u001B[0m     missing_arr \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2326\u001B[0m     missing_arr\u001B[38;5;241m.\u001B[39mfill(fill_value)\n\u001B[0;32m   2328\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmM\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "while len(th_liste) != 0:\n",
    "    th_liste[0].join()\n",
    "    Data = pd.concat([Data, th_liste[0].retour], axis=0)\n",
    "    del th_liste[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T20:32:02.442318900Z",
     "start_time": "2023-08-31T20:05:32.439849400Z"
    }
   },
   "id": "2f19eb84ff32143a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Data = Data.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T20:32:02.438280Z"
    }
   },
   "id": "575e5c96fd86db9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# autre methode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea5883083b5e864"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Data = pd.DataFrame()\n",
    "for i, id in enumerate(list_id_mutation):\n",
    "\n",
    "    tmp_dataset = test.loc[dataset.id_mutation == id]\n",
    "    test = test.drop(list(test.loc[test.id_mutation == id, :].index))\n",
    "    index = tmp_dataset.index[0]\n",
    "\n",
    "    tmp_dict = dict.fromkeys(colonnes)\n",
    "\n",
    "    tmp_dict['date_mutation'] = [tmp_dataset.loc[index, 'date_mutation']]\n",
    "    tmp_dict['code_postal'] = [tmp_dataset.loc[index, 'code_postal']]\n",
    "    tmp_dict['code_commune'] = [tmp_dataset.loc[index, 'code_commune']]\n",
    "    tmp_dict['code_departement'] = [tmp_dataset.loc[index, 'code_departement']]\n",
    "    tmp_dict['nombre_lots'] = [tmp_dataset.loc[index, 'nombre_lots'].sum()]\n",
    "    tmp_dict['surface_reelle_bati'] = [tmp_dataset.loc[index, 'surface_reelle_bati'].sum()]\n",
    "    tmp_dict['nombre_pieces_principales'] = [tmp_dataset.loc[index, 'nombre_pieces_principales'].sum()]\n",
    "    tmp_dict['surface_terrain'] = [tmp_dataset.loc[index, 'surface_terrain'].sum()]\n",
    "    tmp_dict['valeur_fonciere'] = [tmp_dataset.loc[index, 'valeur_fonciere']]\n",
    "    tmp_dict['longitude'] = [tmp_dataset.loc[index, 'longitude']]\n",
    "    tmp_dict['latitude'] = [tmp_dataset.loc[index, 'latitude']]\n",
    "\n",
    "    # valeur classe code type local\n",
    "    tmp_code_type_local = tmp_dataset.loc[:, 'type_local']\n",
    "    for code_type in classe_liste_code_type_local:\n",
    "        tmp_dict[code_type] = [tmp_code_type_local.loc[tmp_code_type_local == code_type].size]\n",
    "\n",
    "    # valeur prefixe voie\n",
    "    tmp_abbrev_voie = tmp_dataset.loc[:, 'prefixe_voie']\n",
    "    for abbrev_voie in classe_liste_prefixe_voie:\n",
    "        tmp_dict[abbrev_voie] = [tmp_abbrev_voie.loc[tmp_abbrev_voie == abbrev_voie].size]\n",
    "\n",
    "    # valeur classe nature mutation\n",
    "    tmp_nature_mutation = tmp_dataset.loc[:, 'nature_mutation']\n",
    "    for nat_mutation in classe_liste_nature_mutation:\n",
    "        tmp_dict[nat_mutation] = [tmp_nature_mutation.loc[tmp_nature_mutation == nat_mutation].size]\n",
    "\n",
    "    # valeur classe culture\n",
    "    tmp_culture = tmp_dataset.loc[:, 'code_nature_culture']\n",
    "    for culture in classe_liste_code_culture:\n",
    "        tmp_dict[culture] = [tmp_culture.loc[tmp_culture == culture].size]\n",
    "\n",
    "    # valeur classe culture spe\n",
    "    tmp_culture_spe = tmp_dataset.loc[:, 'code_nature_culture_speciale']\n",
    "\n",
    "    for culture_spe in classe_liste_code_culture_spe:\n",
    "        tmp_dict[culture_spe] = [tmp_culture_spe.loc[tmp_culture_spe == culture_spe].size]\n",
    "\n",
    "    Data = pd.concat([Data, pd.DataFrame.from_dict(tmp_dict)], ignore_index=True, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T20:32:02.440270400Z"
    }
   },
   "id": "e980bef1632bebff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Data = pd.DataFrame()\n",
    "for i, id in enumerate(list_id_mutation):\n",
    "\n",
    "    tmp_dataset = test.loc[dataset.id_mutation == id]\n",
    "    index = tmp_dataset.index[0]\n",
    "\n",
    "    tmp_dict = dict.fromkeys(colonnes)\n",
    "\n",
    "    tmp_dict['date_mutation'] = [tmp_dataset.loc[index, 'date_mutation']]\n",
    "    tmp_dict['code_postal'] = [tmp_dataset.loc[index, 'code_postal']]\n",
    "    tmp_dict['code_commune'] = [tmp_dataset.loc[index, 'code_commune']]\n",
    "    tmp_dict['code_departement'] = [tmp_dataset.loc[index, 'code_departement']]\n",
    "    tmp_dict['nombre_lots'] = [tmp_dataset.loc[index, 'nombre_lots'].sum()]\n",
    "    tmp_dict['surface_reelle_bati'] = [tmp_dataset.loc[index, 'surface_reelle_bati'].sum()]\n",
    "    tmp_dict['nombre_pieces_principales'] = [tmp_dataset.loc[index, 'nombre_pieces_principales'].sum()]\n",
    "    tmp_dict['surface_terrain'] = [tmp_dataset.loc[index, 'surface_terrain'].sum()]\n",
    "    tmp_dict['valeur_fonciere'] = [tmp_dataset.loc[index, 'valeur_fonciere']]\n",
    "    tmp_dict['longitude'] = [tmp_dataset.loc[index, 'longitude']]\n",
    "    tmp_dict['latitude'] = [tmp_dataset.loc[index, 'latitude']]\n",
    "\n",
    "    # valeur classe code type local\n",
    "    count_type_local = Counter(tmp_dataset.loc[:, 'type_local'])\n",
    "    for type_local in count_type_local:\n",
    "        tmp_dict[type_local] = count_type_local[type_local]\n",
    "\n",
    "    # valeur prefixe voie\n",
    "    count_prefixe_voie = Counter(tmp_dataset.loc[:, 'prefixe_voie'])\n",
    "    for abbrev_voie in count_prefixe_voie:\n",
    "        tmp_dict[abbrev_voie] = count_prefixe_voie[abbrev_voie]\n",
    "\n",
    "    # valeur classe nature mutation\n",
    "    count_nature_mutation = Counter(tmp_dataset.loc[:, 'nature_mutation'])\n",
    "    for nat_mutation in count_nature_mutation:\n",
    "        tmp_dict[nat_mutation] = count_nature_mutation[nat_mutation]\n",
    "\n",
    "    # valeur classe culture\n",
    "    count_type_culture = Counter(tmp_dataset.loc[:, 'code_nature_culture'])\n",
    "    for culture in count_type_culture:\n",
    "        tmp_dict[culture] = count_type_culture[culture]\n",
    "\n",
    "    # valeur classe culture\n",
    "    count_type_culture_spe = Counter(tmp_dataset.loc[:, 'code_nature_culture_speciale'])\n",
    "    for culture_spe in count_type_culture_spe:\n",
    "        tmp_dict[culture_spe] = count_type_culture_spe[culture_spe]\n",
    "\n",
    "    Data = pd.concat([Data, pd.DataFrame.from_dict(tmp_dict)], ignore_index=True, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T20:32:02.446263500Z",
     "start_time": "2023-08-31T20:32:02.446263500Z"
    }
   },
   "id": "b25ec9b0c30b0870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def traitement_id(id: str, dataset: DataFrame, colonnes: np.array, ) -> DataFrame:\n",
    "    tmp_dataset = dataset.loc[dataset.id_mutation == id]\n",
    "    index = tmp_dataset.index[0]\n",
    "\n",
    "    tmp_dict = dict.fromkeys(colonnes)\n",
    "\n",
    "    tmp_dict['date_mutation'] = [tmp_dataset.loc[index, 'date_mutation']]\n",
    "    tmp_dict['code_postal'] = [tmp_dataset.loc[index, 'code_postal']]\n",
    "    tmp_dict['code_commune'] = [tmp_dataset.loc[index, 'code_commune']]\n",
    "    tmp_dict['code_departement'] = [tmp_dataset.loc[index, 'code_departement']]\n",
    "    tmp_dict['nombre_lots'] = [tmp_dataset.loc[index, 'nombre_lots'].sum()]\n",
    "    tmp_dict['surface_reelle_bati'] = [tmp_dataset.loc[index, 'surface_reelle_bati'].sum()]\n",
    "    tmp_dict['nombre_pieces_principales'] = [tmp_dataset.loc[index, 'nombre_pieces_principales'].sum()]\n",
    "    tmp_dict['surface_terrain'] = [tmp_dataset.loc[index, 'surface_terrain'].sum()]\n",
    "    tmp_dict['valeur_fonciere'] = [tmp_dataset.loc[index, 'valeur_fonciere']]\n",
    "    tmp_dict['longitude'] = [tmp_dataset.loc[index, 'longitude']]\n",
    "    tmp_dict['latitude'] = [tmp_dataset.loc[index, 'latitude']]\n",
    "\n",
    "    # valeur classe code type local\n",
    "    count_type_local = Counter(tmp_dataset.loc[:, 'type_local'])\n",
    "    for type_local in count_type_local:\n",
    "        tmp_dict[type_local] = count_type_local[type_local]\n",
    "\n",
    "    # valeur prefixe voie\n",
    "    count_prefixe_voie = Counter(tmp_dataset.loc[:, 'prefixe_voie'])\n",
    "    for abbrev_voie in count_prefixe_voie:\n",
    "        tmp_dict[abbrev_voie] = count_prefixe_voie[abbrev_voie]\n",
    "\n",
    "    # valeur classe nature mutation\n",
    "    count_nature_mutation = Counter(tmp_dataset.loc[:, 'nature_mutation'])\n",
    "    for nat_mutation in count_nature_mutation:\n",
    "        tmp_dict[nat_mutation] = count_nature_mutation[nat_mutation]\n",
    "\n",
    "    # valeur classe culture\n",
    "    count_type_culture = Counter(tmp_dataset.loc[:, 'code_nature_culture'])\n",
    "    for culture in count_type_culture:\n",
    "        tmp_dict[culture] = count_type_culture[culture]\n",
    "\n",
    "    # valeur classe culture\n",
    "    count_type_culture_spe = Counter(tmp_dataset.loc[:, 'code_nature_culture_speciale'])\n",
    "    for culture_spe in count_type_culture_spe:\n",
    "        tmp_dict[culture_spe] = count_type_culture_spe[culture_spe]\n",
    "\n",
    "    return pd.DataFrame.from_dict(tmp_dict).fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-31T20:32:02.446263500Z"
    }
   },
   "id": "c76bc80b4cf5e2a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sortie de la colonne resultat\n",
    "\n",
    "## Y = df.valeur_fonciere"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-31T20:32:02.459270500Z",
     "start_time": "2023-08-31T20:32:02.447265600Z"
    }
   },
   "id": "1b3922c306493006"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
